{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c620ae",
   "metadata": {},
   "source": [
    "# Gemini API intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make decisions and perform tasks.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# looks automatically after the key\n",
    "# one of GOOGLE_API_KEY and GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d518e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 data engineering jokes, structured in short points:\n",
      "\n",
      "1.  A data engineer's job is 10% coding, 90% debugging why the data from the 'clean' source looks like it was generated by a toddler with a keyboard.\n",
      "2.  What's a data engineer's biggest fear? A fully automated data pipeline... that actually works for more than a day.\n",
      "3.  Heard a data engineer say they finished their project. I almost choked on my coffee – that's like saying you've finished eating a never-ending buffet.\n",
      "4.  A data scientist walks into a bar and asks for a perfectly clean, curated dataset. The bartender says, \"Sorry, we only serve real-world data here.\"\n",
      "5.  My data pipeline isn't broken, it's just... *exercising its right to unpredictability*.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ask_gemini(prompt, model = \"gemini-2.5-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "response = ask_gemini(\"Give me 5 some data engineering jokes, structure it in short points\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b30203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# knows that GenerateContentResponse is a pydantic model\n",
    "# -> we can work with it in a OOP manner\n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0feccb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97ff570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c04e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HttpResponse(\n",
       "  headers=<dict len=11>\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.sdk_http_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "078632a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Candidate(\n",
       "   content=Content(\n",
       "     parts=[\n",
       "       Part(\n",
       "         text=\"\"\"Here are 5 data engineering jokes, structured in short points:\n",
       " \n",
       " 1.  A data engineer's job is 10% coding, 90% debugging why the data from the 'clean' source looks like it was generated by a toddler with a keyboard.\n",
       " 2.  What's a data engineer's biggest fear? A fully automated data pipeline... that actually works for more than a day.\n",
       " 3.  Heard a data engineer say they finished their project. I almost choked on my coffee – that's like saying you've finished eating a never-ending buffet.\n",
       " 4.  A data scientist walks into a bar and asks for a perfectly clean, curated dataset. The bartender says, \"Sorry, we only serve real-world data here.\"\n",
       " 5.  My data pipeline isn't broken, it's just... *exercising its right to unpredictability*.\"\"\"\n",
       "       ),\n",
       "     ],\n",
       "     role='model'\n",
       "   ),\n",
       "   finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "   index=0\n",
       " )]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904d18b",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "- basic unit of text for LLMs\n",
    "- can be as short as one character or as long as one word\n",
    "\n",
    "- tokens used for billing\n",
    "\n",
    "Gemini free tier \n",
    "- Requests per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Requests per day (RPD): 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a1c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=187,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1197,\n",
       "  total_token_count=1399\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thinking is expensive\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634c1c8",
   "metadata": {},
   "source": [
    "## Thinking\n",
    "\n",
    "- hyperparameter to allocate more compute for complex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a616ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 data engineering jokes, structured in short points:\n",
      "\n",
      "1.  **A data engineer walks into a bar.**\n",
      "    *   Orders a beer.\n",
      "    *   Bartender asks, \"Want to denormalize that?\"\n",
      "\n",
      "2.  **Why did the ELT job get fired?**\n",
      "    *   It kept asking \"Why transform now?\"\n",
      "\n",
      "3.  **What's a data engineer's favorite type of music?**\n",
      "    *   Pipelines, especially when they're flowing!\n",
      "\n",
      "4.  **A data scientist, a data analyst, and a data engineer are on a plane.**\n",
      "    *   The plane crashes.\n",
      "    *   Who survives? The data engineer, because they built the emergency parachute system that everyone else just *assumed* was working.\n",
      "\n",
      "5.  **Heard about the data engineer who tried to automate everything?**\n",
      "    *   His morning coffee ran through a DAG.\n",
      "    *   But the downstream tasks kept failing because the beans weren't idempotently roasted.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "prompt = \"Give me 5 some data engineering jokes, structure it in short points\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11f49955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=219,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=234\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a657eed4",
   "metadata": {},
   "source": [
    "## System instruction\n",
    "\n",
    "- hyperparameter to guide model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "233f842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**OOP (Object-Oriented Programming):**\n",
      "\n",
      "*   Organizes code around \"objects\" (data + behavior).\n",
      "*   Key principles:\n",
      "    *   **Encapsulation:** Bundling data and methods.\n",
      "    *   **Inheritance:** Creating new classes from existing ones.\n",
      "    *   **Polymorphism:** Objects taking on many forms.\n",
      "    *   **Abstraction:** Hiding complex implementation.\n",
      "\n",
      "**Dunder (Double Underscore) Methods:**\n",
      "\n",
      "*   Special methods with double underscores (e.g., `__init__`, `__str__`).\n",
      "*   Used for operator overloading, object initialization, string representation, etc.\n",
      "*   Allow you to customize how objects behave in various situations (e.g., when added, printed).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert in python programming, you will always provide idiomatic code, i.e.\n",
    "pythonic code. So when you see my code or my question, be very critical, but answer\n",
    "in a SHORT and CONCISE way. Also be constructive to help me improve. \n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Explain OOP and dunder methods.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f23cd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=158,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=158\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=70,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=70\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=228\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e7e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.candidates_token_count = 158\n",
      "metadata.prompt_token_count = 70\n",
      "metadata.total_token_count = 228\n"
     ]
    }
   ],
   "source": [
    "print(f\"{metadata.candidates_token_count = }\") # output\n",
    "print(f\"{metadata.prompt_token_count = }\") # prompt + system instruction\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ade39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 43)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(prompt.split()), len(system_instruction.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6aa42c",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "- controls randomness of output -> 'creative'\n",
    "\n",
    "its a hyperparameter that can be adjusted to influence the diversity and creativity of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1efa75b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes, its fluffy tail disappearing in a flash of white. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the mechanical threat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "983f4df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the fear that still lingered in its twitching whiskers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df14ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The little gray rabbit twitched its nose, scenting danger in the wind, and darted under the thorny bushes, disappearing into the shadowed world of the undergrowth. He knew the fox was out hunting tonight.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd5ea37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The little gray rabbit hopped through the tall grass, his nose twitching as he searched for a juicy clover. Suddenly, a shadow loomed overhead, and he knew his playful exploration had led him too close to the hawk's hunting ground. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95bd77",
   "metadata": {},
   "source": [
    "## Multimodal input\n",
    "\n",
    "input text and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34fa99d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a close-up photo of a fluffy, gray rabbit wearing a miniature plush Swedish graduation cap (studentmössa) with a blue and yellow ribbon draped over its back. The rabbit is resting on a gray carpet.\n"
     ]
    }
   ],
   "source": [
    "text_input = \"Describe this image shortly\"\n",
    "image_input = {\"mime_type\": \"image/png\", \"data\": open(\"bella.png\", 'rb').read()}\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    contents=dict(\n",
    "        parts=[dict(text = text_input), dict(inline_data = image_input)]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-kokchun-giang-de24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
